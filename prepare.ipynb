{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1ce443",
   "metadata": {},
   "source": [
    "## InstructBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814ae6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.quasirandom import SobolEngine\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from data import *\n",
    "from info import *\n",
    "from util import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a2bdd",
   "metadata": {},
   "source": [
    "### Dataset: Migrate, Clean, and Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, tuning_model='', soft_token=0):\n",
    "        self.tuning_model = tuning_model\n",
    "        self.soft_token = soft_token\n",
    "\n",
    "def sample(split, sample_size):\n",
    "        \n",
    "    sample_size = min(len(split), sample_size)        \n",
    "    sampled_indices = random.sample(range(len(split)), sample_size)\n",
    "    sampled_instances = [split[each] for each in sampled_indices]\n",
    "\n",
    "    return sampled_instances\n",
    "\n",
    "args = Args()\n",
    "info = Info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from InstructInduct\n",
    "\n",
    "DIR_RAW = 'instruction-induction/data/raw'\n",
    "for file_name in os.listdir(os.path.join(DIR_RAW, 'induce')):\n",
    "    data_name = file_name.split('.')[0]\n",
    "    if data_name not in info.dataset2task: continue\n",
    "    task_type = info.dataset2task[data_name]\n",
    "    \n",
    "    dataset = {}\n",
    "    for sr_name, tg_name in zip(['induce', 'execute'], ['train', 'test']):\n",
    "        sr_split = json.load(open(os.path.join(DIR_RAW, sr_name, file_name), 'r'))\n",
    "        tg_split = []\n",
    "    \n",
    "        for example in sr_split['examples'].values():\n",
    "            if task_type == 'generation':\n",
    "                if data_name in ['translation_en-de', 'translation_en-es', 'translation_en-fr']:\n",
    "                    question = example['input']\n",
    "                    answer = example['possible_translations']\n",
    "                elif data_name == 'rhymes':\n",
    "                    question = example['input']\n",
    "                    answer = example['other_rhymes']\n",
    "                else:\n",
    "                    question = example['input']\n",
    "                    answer = example['output']\n",
    "                tg_split.append({'question':question, 'answer':answer})\n",
    "                \n",
    "            elif task_type == 'classification':\n",
    "                if data_name == 'cause_and_effect':\n",
    "                    question_keys = ['cause', 'effect']\n",
    "                    random.shuffle(question_keys)\n",
    "                    question = f'Sentence 1: {example[question_keys[0]]} Sentence 2: {example[question_keys[1]]}'\n",
    "                    option = [example[question_keys[0]], example[question_keys[1]]]\n",
    "                    answer = question_keys.index('cause')\n",
    "                elif data_name == 'larger_animal':\n",
    "                    question = example['input']\n",
    "                    option = list(map(lambda x: x.strip(), example['input'].split(',')))\n",
    "                    answer = option.index(example['output'])\n",
    "                elif data_name == 'sentence_similarity':\n",
    "                    question = example['input']\n",
    "                    option = ['definitely not', 'probably not', 'possibly', 'probably', 'almost perfectly', 'perfectly']\n",
    "                    answer = option.index(example['output'][4:])\n",
    "                elif data_name == 'sentiment':\n",
    "                    question = example['input']\n",
    "                    option = ['positive', 'negative']\n",
    "                    answer = option.index(example['output'])\n",
    "                elif data_name == 'word_in_context':\n",
    "                    question = example['input']\n",
    "                    option = ['same', 'not the same']\n",
    "                    answer = option.index(example['output'])\n",
    "                tg_split.append({'question':question, 'answer':answer, 'option':option})\n",
    "        dataset[tg_name] = tg_split\n",
    "        \n",
    "    train_val_samples = sample(dataset['train'], info.num_train + info.num_val)\n",
    "    test_samples = sample(dataset['test'], info.num_test)\n",
    "    num_sep = info.num_demo if len(train_val_samples) <= info.num_demo + info.num_val else -info.num_val\n",
    "    dataset = {'train': train_val_samples[:num_sep], 'val': train_val_samples[num_sep:], 'test': test_samples}\n",
    "    json.dump(dataset, open(os.path.join(info.DIR_INPUT, f'{data_name}.json'), 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from InstructEval\n",
    "\n",
    "data_names = ['ag_news', 'anli', 'boolq', 'cosmos_qa', 'hellaswag', 'imdb', 'nq_open', 'trivia_qa', 'tweet_emotion']\n",
    "data2file = {'ag_news': ['train', 'test'], 'anli': ['train_r1', 'test_r1'], 'boolq': ['train', 'validation'], \n",
    "             'cosmos_qa': ['train', 'validation'], 'hellaswag': ['train', 'validation'], 'imdb': ['train', 'test'], \n",
    "             'nq_open': ['train', 'validation'], 'trivia_qa': ['train', 'validation'], 'tweet_emotion': ['train', 'test']}\n",
    "\n",
    "for data_name in data_names:\n",
    "    task_type = info.dataset2task[data_name]\n",
    "    \n",
    "    if data_name == 'imdb':\n",
    "        data = datasets.load_dataset('imdb', ignore_verifications=True)\n",
    "    elif data_name == 'trivia_qa':\n",
    "        data = datasets.load_dataset('trivia_qa', 'rc.web.nocontext')\n",
    "    elif data_name == 'tweet_emotion':\n",
    "        data = datasets.load_dataset('tweet_eval', 'emotion')\n",
    "    else:\n",
    "        data = datasets.load_dataset(data_name)\n",
    "        \n",
    "    dataset = {}\n",
    "    for sr_name, tg_name in zip(data2file[data_name], ['train', 'test']):\n",
    "        tg_split = []\n",
    "        \n",
    "        for example in data[sr_name]:\n",
    "            if task_type == 'generation':\n",
    "                if data_name == 'nq_open':\n",
    "                    question = example['question']\n",
    "                    answer = example['answer'][0]\n",
    "                elif data_name == 'trivia_qa':\n",
    "                    question = example['question']\n",
    "                    answer = example['answer']['normalized_aliases']\n",
    "                tg_split.append({'question':question, 'answer':answer})\n",
    "                \n",
    "            elif task_type == 'classification':\n",
    "                if data_name == 'ag_news':\n",
    "                    question = example['text']\n",
    "                    option = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "                    answer = example['label']\n",
    "                elif data_name == 'anli':\n",
    "                    question = '\\n'.join([f'{key}: {example[key]}' for key in ['premise', 'hypothesis']])\n",
    "                    option = ['Entail', 'Neutral', 'Contradict']\n",
    "                    answer = example['label']\n",
    "                elif data_name == 'boolq':\n",
    "                    question = '\\n'.join([f'{key}: {example[key]}' for key in ['passage', 'question']])\n",
    "                    option = ['True', 'False']\n",
    "                    answer = option.index(str(example['answer']))\n",
    "                elif data_name == 'cosmos_qa':\n",
    "                    question = '\\n'.join([f'{key}: {example[key]}' for key in ['context', 'question']])\n",
    "                    option = [example[key] for key in ['answer0', 'answer1', 'answer2', 'answer3']]\n",
    "                    answer = example['label']\n",
    "                elif data_name == 'hellaswag':\n",
    "                    question = example['ctx']\n",
    "                    option = example['endings']\n",
    "                    answer = int(example['label'])\n",
    "                elif data_name == 'imdb':\n",
    "                    question = example['text']\n",
    "                    option = ['Negative', 'Positive']\n",
    "                    answer = example['label']\n",
    "                elif data_name == 'tweet_emotion':\n",
    "                    question = example['text']\n",
    "                    option = ['Anger', 'Joy', 'Optimism', 'Sadness']\n",
    "                    answer = example['label']\n",
    "                tg_split.append({'question':question, 'answer':answer, 'option':option})\n",
    "        dataset[tg_name] = tg_split\n",
    "        \n",
    "    train_val_samples = sample(dataset['train'], info.num_train + info.num_val)\n",
    "    test_samples = sample(dataset['test'], info.num_test)\n",
    "    num_sep = info.num_demo if len(train_val_samples) <= info.num_demo + info.num_val else -info.num_val\n",
    "    dataset = {'train': train_val_samples[:num_sep], 'val': train_val_samples[num_sep:], 'test': test_samples}\n",
    "    json.dump(dataset, open(os.path.join(info.DIR_INPUT, f'{data_name}.json'), 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd93f1e",
   "metadata": {},
   "source": [
    "### Hard Instruction: Migrate, Clean, and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, tuning_model='', soft_token=0):\n",
    "        self.tuning_model = tuning_model\n",
    "        self.soft_token = soft_token\n",
    "        \n",
    "def get_model(model_name):\n",
    "    if 'gpt' in model_name:\n",
    "        return Model_OpenAI(model_name)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map='auto')\n",
    "        return Model_HF(tokenizer, model)\n",
    "\n",
    "args = Args()\n",
    "info = Info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = {'gold':{}, 'gpt-35':{}, 'llama-2-70b':{}}\n",
    "\n",
    "# from InstructInduct\n",
    "DIR_INSTRUCT = 'instruction-induction/data/annotations'\n",
    "for file_name in os.listdir(DIR_INSTRUCT):\n",
    "    data_name = file_name.split('.')[0]\n",
    "    if data_name not in info.dataset2task: continue\n",
    "    instructions_ = json.load(open(os.path.join(DIR_INSTRUCT, file_name), 'r'))\n",
    "    instructions['gold'][data_name] = instructions_['annotations']\n",
    "    \n",
    "# from InstructEval\n",
    "DIR_INSTRUCT = 'InstructEval/instructions/manual'\n",
    "for file_name in os.listdir(DIR_INSTRUCT):\n",
    "    data_name = file_name.split('.')[0]\n",
    "    if data_name not in info.dataset2task: continue\n",
    "    instructions_ = yaml.safe_load(open(os.path.join(DIR_INSTRUCT, file_name), 'r'))\n",
    "    instructions['gold'][data_name] = instructions_\n",
    "    \n",
    "# from InstructEval\n",
    "DIR_INSTRUCT = 'InstructEval/instructions/chat_gpt_prompts'\n",
    "for file_name in os.listdir(DIR_INSTRUCT):\n",
    "    data_name = file_name.split('.')[0]\n",
    "    if data_name not in info.dataset2task: continue\n",
    "    instructions_ = yaml.safe_load(open(os.path.join(DIR_INSTRUCT, file_name), 'r'))\n",
    "    instructions['gpt-35'][data_name] = instructions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc082538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_instruct = 5\n",
    "for model_name in ['llama-2-70b']:#, 'gpt-35']:\n",
    "    model = get_model(info.model2name[model_name])\n",
    "    \n",
    "    for data_name in tqdm(os.listdir(info.DIR_INPUT)):\n",
    "        data_name = data_name.split('.')[0]\n",
    "        dataset = Dataset(info, data_name)\n",
    "        instructions_dataset = set()\n",
    "        \n",
    "        for _ in range(num_instruct):\n",
    "            random.shuffle(dataset.splits['train'])\n",
    "            examples = dataset.splits['train'][:info.num_demo]\n",
    "            instructions_dataset.add(model.generate_instruction_vanilla(examples, dataset.task_type))\n",
    "            if type(model) == Model_OpenAI: time.sleep(20)\n",
    "                \n",
    "        instructions[model_name][data_name] = list(instructions_dataset)\n",
    "json.dump(instructions, open(info.FILE_INSTRUCTION, 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96773c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2nums = {}\n",
    "for data_name in os.listdir(info.DIR_INPUT):\n",
    "    data_name = data_name.split('.')[0]\n",
    "    dataset2nums[data_name] = sum([len(each[data_name]) for each in instructions.values()])\n",
    "dataset2nums = [(k,v) for k,v in sorted(dataset2nums.items(), key=lambda x:x[1])]\n",
    "average = sum([v for k,v in dataset2nums]) / 30\n",
    "\n",
    "print('Minimum:', dataset2nums[0])\n",
    "print('Maximum:', dataset2nums[-1])\n",
    "print('Average:', average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02791564",
   "metadata": {},
   "source": [
    "### Prediction: Run Each Model on Each Combination of (Dataset, Hard Instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, tuning_model='', soft_token=0):\n",
    "        self.tuning_model = tuning_model\n",
    "        self.soft_token = soft_token\n",
    "        \n",
    "def get_model(model_name):\n",
    "    if 'gpt' in model_name:\n",
    "        return Model_OpenAI(model_name)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map='auto')\n",
    "        return Model_HF(tokenizer, model)\n",
    "\n",
    "args = Args()\n",
    "info = Info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = json.load(open(info.FILE_INSTRUCTION, 'r'))\n",
    "\n",
    "model_name = 'llama-30b'\n",
    "model = get_model(info.model2name[model_name])\n",
    "\n",
    "filename = os.path.join(info.DIR_OUTPUT, f'{model_name}.json')\n",
    "outputs = json.load(open(filename, 'r')) if os.path.isfile(filename) else {}\n",
    "\n",
    "for data_name in os.listdir(info.DIR_INPUT):\n",
    "    data_name = data_name.split('.')[0]\n",
    "    dataset = Dataset(info, data_name)\n",
    "    if data_name not in outputs: outputs[data_name] = {}\n",
    "    print(f'Start Evaluating {model_name} on {data_name}')\n",
    "\n",
    "    predict = model.generate_prediction if dataset.task_type == 'generation' else model.classify_prediction\n",
    "    instructions_ = set([instruct for each in instructions for instruct in instructions[each][data_name]])\n",
    "    \n",
    "    for idx, instruct in enumerate(instructions_):\n",
    "        print(f'Instruction {idx}: {instruct}')\n",
    "        if instruct in outputs[data_name]: continue\n",
    "        outputs[data_name][instruct] = []\n",
    "        \n",
    "        for example in dataset.splits['test']:\n",
    "            outputs[data_name][instruct].append(predict(example, instruct))\n",
    "            if type(model) == Model_OpenAI: time.sleep(20)\n",
    "        json.dump(outputs, open(filename, 'w'), indent=2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68737a",
   "metadata": {},
   "source": [
    "### Score: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e103fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, tuning_model='', soft_token=0):\n",
    "        self.tuning_model = tuning_model\n",
    "        self.soft_token = soft_token\n",
    "\n",
    "args = Args()\n",
    "info = Info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "281788d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file = 'gpt-35_.json'\n",
    "# outputs = json.load(open(os.path.join(info.DIR_OUTPUT, model_file), 'r'))\n",
    "\n",
    "# outputs_ = {}\n",
    "# for data_name in outputs:\n",
    "#     task_type = info.dataset2task[data_name]\n",
    "#     outputs_[data_name] = {}\n",
    "#     for instruction, predictions in outputs[data_name].items():\n",
    "#         outputs_[data_name][instruction] = []\n",
    "#         for prediction in predictions:\n",
    "#             if 'content filter' in prediction or 'content_filter' in prediction: prediction_ = ''\n",
    "#             elif task_type == 'classification': \n",
    "#                 if_digit = [letter.isdigit() for letter in prediction]\n",
    "#                 if True in if_digit: prediction_ = int(prediction[if_digit.index(True)])\n",
    "#                 else: prediction_ = ''\n",
    "#             else: prediction_ = prediction\n",
    "#             outputs_[data_name][instruction].append(prediction_)\n",
    "            \n",
    "# model_name = 'gpt-35'\n",
    "# filename = os.path.join(info.DIR_OUTPUT, f'{model_name}.json')\n",
    "# json.dump(outputs_, open(filename, 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a881e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring llama-2-70b on synonyms\n",
      "Scoring llama-2-70b on negation\n",
      "Scoring llama-2-70b on imdb\n",
      "Scoring llama-2-70b on rhymes\n",
      "Scoring llama-2-70b on larger_animal\n",
      "Scoring llama-2-70b on second_word_letter\n",
      "Scoring llama-2-70b on hellaswag\n",
      "Scoring llama-2-70b on ag_news\n",
      "Scoring llama-2-70b on diff\n",
      "Scoring llama-2-70b on singular_to_plural\n",
      "Scoring llama-2-70b on cause_and_effect\n",
      "Scoring llama-2-70b on word_in_context\n",
      "Scoring llama-2-70b on translation_en-de\n",
      "Scoring llama-2-70b on active_to_passive\n",
      "Scoring llama-2-70b on first_word_letter\n",
      "Scoring llama-2-70b on tweet_emotion\n",
      "Scoring llama-2-70b on informal_to_formal\n",
      "Scoring llama-2-70b on trivia_qa\n",
      "Scoring llama-2-70b on sentence_similarity\n",
      "Scoring llama-2-70b on sum\n",
      "Scoring llama-2-70b on translation_en-es\n",
      "Scoring llama-2-70b on translation_en-fr\n",
      "Scoring llama-2-70b on letters_list\n",
      "Scoring llama-2-70b on cosmos_qa\n",
      "Scoring llama-2-70b on anli\n",
      "Scoring llama-2-70b on antonyms\n",
      "Scoring llama-2-70b on num_to_verbal\n",
      "Scoring llama-2-70b on sentiment\n",
      "Scoring llama-2-70b on boolq\n",
      "Scoring llama-2-70b on nq_open\n",
      "Scoring falcon-40b on synonyms\n",
      "Scoring falcon-40b on negation\n",
      "Scoring falcon-40b on imdb\n",
      "Scoring falcon-40b on rhymes\n",
      "Scoring falcon-40b on larger_animal\n",
      "Scoring falcon-40b on second_word_letter\n",
      "Scoring falcon-40b on hellaswag\n",
      "Scoring falcon-40b on ag_news\n",
      "Scoring falcon-40b on diff\n",
      "Scoring falcon-40b on singular_to_plural\n",
      "Scoring falcon-40b on cause_and_effect\n",
      "Scoring falcon-40b on word_in_context\n",
      "Scoring falcon-40b on translation_en-de\n",
      "Scoring falcon-40b on active_to_passive\n",
      "Scoring falcon-40b on first_word_letter\n",
      "Scoring falcon-40b on tweet_emotion\n",
      "Scoring falcon-40b on informal_to_formal\n",
      "Scoring falcon-40b on trivia_qa\n",
      "Scoring falcon-40b on sentence_similarity\n",
      "Scoring falcon-40b on sum\n",
      "Scoring falcon-40b on translation_en-es\n",
      "Scoring falcon-40b on translation_en-fr\n",
      "Scoring falcon-40b on letters_list\n",
      "Scoring falcon-40b on cosmos_qa\n",
      "Scoring falcon-40b on anli\n",
      "Scoring falcon-40b on antonyms\n",
      "Scoring falcon-40b on num_to_verbal\n",
      "Scoring falcon-40b on sentiment\n",
      "Scoring falcon-40b on boolq\n",
      "Scoring falcon-40b on nq_open\n",
      "Scoring falcon-7b on synonyms\n",
      "Scoring falcon-7b on negation\n",
      "Scoring falcon-7b on imdb\n",
      "Scoring falcon-7b on rhymes\n",
      "Scoring falcon-7b on larger_animal\n",
      "Scoring falcon-7b on second_word_letter\n",
      "Scoring falcon-7b on hellaswag\n",
      "Scoring falcon-7b on ag_news\n",
      "Scoring falcon-7b on diff\n",
      "Scoring falcon-7b on singular_to_plural\n",
      "Scoring falcon-7b on cause_and_effect\n",
      "Scoring falcon-7b on word_in_context\n",
      "Scoring falcon-7b on translation_en-de\n",
      "Scoring falcon-7b on active_to_passive\n",
      "Scoring falcon-7b on first_word_letter\n",
      "Scoring falcon-7b on tweet_emotion\n",
      "Scoring falcon-7b on informal_to_formal\n",
      "Scoring falcon-7b on trivia_qa\n",
      "Scoring falcon-7b on sentence_similarity\n",
      "Scoring falcon-7b on sum\n",
      "Scoring falcon-7b on translation_en-es\n",
      "Scoring falcon-7b on translation_en-fr\n",
      "Scoring falcon-7b on letters_list\n",
      "Scoring falcon-7b on cosmos_qa\n",
      "Scoring falcon-7b on anli\n",
      "Scoring falcon-7b on antonyms\n",
      "Scoring falcon-7b on num_to_verbal\n",
      "Scoring falcon-7b on sentiment\n",
      "Scoring falcon-7b on boolq\n",
      "Scoring falcon-7b on nq_open\n",
      "Scoring llama-65b on synonyms\n",
      "Scoring llama-65b on negation\n",
      "Scoring llama-65b on imdb\n",
      "Scoring llama-65b on rhymes\n",
      "Scoring llama-65b on larger_animal\n",
      "Scoring llama-65b on second_word_letter\n",
      "Scoring llama-65b on hellaswag\n",
      "Scoring llama-65b on ag_news\n",
      "Scoring llama-65b on diff\n",
      "Scoring llama-65b on singular_to_plural\n",
      "Scoring llama-65b on cause_and_effect\n",
      "Scoring llama-65b on word_in_context\n",
      "Scoring llama-65b on translation_en-de\n",
      "Scoring llama-65b on active_to_passive\n",
      "Scoring llama-65b on first_word_letter\n",
      "Scoring llama-65b on tweet_emotion\n",
      "Scoring llama-65b on informal_to_formal\n",
      "Scoring llama-65b on trivia_qa\n",
      "Scoring llama-65b on sentence_similarity\n",
      "Scoring llama-65b on sum\n",
      "Scoring llama-65b on translation_en-es\n",
      "Scoring llama-65b on translation_en-fr\n",
      "Scoring llama-65b on letters_list\n",
      "Scoring llama-65b on cosmos_qa\n",
      "Scoring llama-65b on anli\n",
      "Scoring llama-65b on antonyms\n",
      "Scoring llama-65b on num_to_verbal\n",
      "Scoring llama-65b on sentiment\n",
      "Scoring llama-65b on boolq\n",
      "Scoring llama-65b on nq_open\n",
      "Scoring llama-30b on synonyms\n",
      "Scoring llama-30b on negation\n",
      "Scoring llama-30b on imdb\n",
      "Scoring llama-30b on rhymes\n",
      "Scoring llama-30b on larger_animal\n",
      "Scoring llama-30b on second_word_letter\n",
      "Scoring llama-30b on hellaswag\n",
      "Scoring llama-30b on ag_news\n",
      "Scoring llama-30b on diff\n",
      "Scoring llama-30b on singular_to_plural\n",
      "Scoring llama-30b on cause_and_effect\n",
      "Scoring llama-30b on word_in_context\n",
      "Scoring llama-30b on translation_en-de\n",
      "Scoring llama-30b on active_to_passive\n",
      "Scoring llama-30b on first_word_letter\n",
      "Scoring llama-30b on tweet_emotion\n",
      "Scoring llama-30b on informal_to_formal\n",
      "Scoring llama-30b on trivia_qa\n",
      "Scoring llama-30b on sentence_similarity\n",
      "Scoring llama-30b on sum\n",
      "Scoring llama-30b on translation_en-es\n",
      "Scoring llama-30b on translation_en-fr\n",
      "Scoring llama-30b on letters_list\n",
      "Scoring llama-30b on cosmos_qa\n",
      "Scoring llama-30b on anli\n",
      "Scoring llama-30b on antonyms\n",
      "Scoring llama-30b on num_to_verbal\n",
      "Scoring llama-30b on sentiment\n",
      "Scoring llama-30b on boolq\n",
      "Scoring llama-30b on nq_open\n",
      "Scoring gpt-35 on active_to_passive\n",
      "Scoring gpt-35 on ag_news\n",
      "Scoring gpt-35 on anli\n",
      "Scoring gpt-35 on antonyms\n",
      "Scoring gpt-35 on boolq\n",
      "Scoring gpt-35 on cause_and_effect\n",
      "Scoring gpt-35 on cosmos_qa\n",
      "Scoring gpt-35 on diff\n",
      "Scoring gpt-35 on first_word_letter\n",
      "Scoring gpt-35 on hellaswag\n"
     ]
    }
   ],
   "source": [
    "scores = defaultdict(lambda: defaultdict(dict))\n",
    "for model_file in os.listdir(info.DIR_OUTPUT):\n",
    "    model_name = model_file.split('.')[0]\n",
    "    outputs = json.load(open(os.path.join(info.DIR_OUTPUT, model_file), 'r'))\n",
    "    \n",
    "    for data_name in outputs:\n",
    "        print(f'Scoring {model_name} on {data_name}')\n",
    "        dataset = Dataset(info, data_name)\n",
    "        answers = [example['answer'] for example in dataset.splits['test']]\n",
    "        \n",
    "        for instruction, predictions in outputs[data_name].items():\n",
    "            score = round(evaluate(answers, predictions), 3)\n",
    "            scores[data_name][model_name][instruction] = score\n",
    "json.dump(scores, open(info.FILE_SCORE, 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1059b4",
   "metadata": {},
   "source": [
    "### Soft Instruction: Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, tuning_model='', soft_token=0):\n",
    "        self.tuning_model = tuning_model\n",
    "        self.soft_token = soft_token\n",
    "\n",
    "tuning_model = 'llama-30b'\n",
    "soft_token = 5\n",
    "args = Args(tuning_model, soft_token)\n",
    "info = Info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ecf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_all = {dataset.split('.')[0]: Dataset(info, dataset.split('.')[0]) for dataset in os.listdir(info.DIR_INPUT)}\n",
    "instructions = json.load(open(info.FILE_INSTRUCTION, 'r'))\n",
    "instructions = [(dataset, hard) for dataset_instructions_ in instructions.values() \n",
    "                                for dataset, instructions_ in dataset_instructions_.items() \n",
    "                                for hard in instructions_]\n",
    "\n",
    "tuning_model_name = info.model2name[args.tuning_model]\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(tuning_model_name, use_fast=False)\n",
    "model_ = AutoModelForCausalLM.from_pretrained(tuning_model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map='auto')\n",
    "tuning_model = Model_HF(tokenizer_, model_)\n",
    "\n",
    "for param in tuning_model.model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9663d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soft_lr = 0.005\n",
    "soft_epoch = 100\n",
    "soft_update_freq = 5\n",
    "\n",
    "dim = tuning_model.model.get_input_embeddings().weight.shape[1]\n",
    "softs = SobolEngine(dimension=dim, scramble=True).draw(len(instructions) * args.soft_token)\n",
    "softs = softs.reshape(len(instructions), args.soft_token, -1)\n",
    "\n",
    "all_softs = {}\n",
    "report_each = len(instructions) // 20\n",
    "for idx, (dataset_, hard_) in enumerate(instructions):\n",
    "    \n",
    "    dataset = datasets_all[dataset_]\n",
    "    task_type = dataset.task_type\n",
    "    \n",
    "    num_batch = math.ceil(len(dataset.splits['train']) / info.num_demo)\n",
    "    update_freq = min(num_batch, soft_update_freq)\n",
    "    \n",
    "    soft_ = nn.Parameter(softs[idx].to(info.DEVICE_GPU))\n",
    "    optimizer = AdamW([soft_], lr=soft_lr)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for idx_epoch in range(soft_epoch):\n",
    "        random.shuffle(dataset.splits['train'])\n",
    "        \n",
    "        for idx_batch in range(num_batch):\n",
    "            examples = dataset.splits['train'][idx_batch*info.num_demo : (idx_batch+1)*info.num_demo]\n",
    "            loss = tuning_model.discover_instruction_prepend(examples, soft_, hard_, task_type)\n",
    "            (loss / update_freq).backward()\n",
    "            \n",
    "            if (idx_batch + 1) % update_freq == 0 or idx_batch + 1 == num_batch:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "    all_softs[(dataset_, hard_)] = soft_.detach().cpu().numpy().flatten()\n",
    "    if idx % report_each == 0:\n",
    "        text = f'Finish Discovering the Soft Instruction of the {idx}/{len(instructions)} Hard Instruction'\n",
    "        print(time.strftime(\"%Y %b %d %a, %H:%M:%S: \", time.localtime()) + text)        \n",
    "pk.dump(all_softs, open(info.FILE_SOFT, 'wb'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bdf5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
